{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f49e536",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "Take the Restaurant dataset given in google classroom\n",
    "\n",
    "\n",
    "# Question 1\n",
    "Prepare the data for modelling, This will include\n",
    "- Converting to lowercase\n",
    "- Removing stopwords\n",
    "- Lemmatization\n",
    "- Stemming\n",
    "- Removing numbers \n",
    "\n",
    "### Brownie points if you can do it all in 1 function\n",
    "\n",
    "# Question 2\n",
    "Once the data has been cleaned :-\n",
    "- Divide the data intro train and test parts\n",
    "- Convert the training and text text into vectors \n",
    "- Convert the labels intro numbers(encoding)\n",
    "\n",
    "Overall, you should end up with 4 varaibles \n",
    "- Xtrain\n",
    "- Xtest\n",
    "- Ytrain\n",
    "- Ytest\n",
    "\n",
    "You can name these whatever you want, upto you. But i am looking for these 4 variables ready to go for modelling\n",
    "\n",
    "------------\n",
    "\n",
    "![](https://media.tenor.com/d8fG2J6pkAUAAAAC/friends-chandler.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0058752",
   "metadata": {},
   "source": [
    "# A hint\n",
    "The file is stores as a ```tsv```.\n",
    "\n",
    "You can load a ```tsv``` file with ```pd.read_csv``` while passing the seperator or ```sep``` argument as ```\\t```.\n",
    "\n",
    "This tells pandas that the file is seperated by tab's instead of comma's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54e0aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb89bc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Restaurant_Reviews - Restaurant_Reviews.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f932db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Review  Liked\n",
       "0                             Wow... Loved this place.      1\n",
       "1                                   Crust is not good.      0\n",
       "2            Not tasty and the texture was just nasty.      0\n",
       "3    Stopped by during the late May bank holiday of...      1\n",
       "4    The selection on the menu was great and so wer...      1\n",
       "..                                                 ...    ...\n",
       "995  I think food should have flavor and texture an...      0\n",
       "996                           Appetite instantly gone.      0\n",
       "997  Overall I was not impressed and would not go b...      0\n",
       "998  The whole experience was underwhelming, and I ...      0\n",
       "999  Then, as if I hadn't wasted enough of my life ...      0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037bb08c",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae82f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary library.\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "sn_stemmer=SnowballStemmer(\"english\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de544c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datacleaning(x):\n",
    "#---------------------------------------------------------------------------------------------------------------------------    \n",
    "    #Removing stop words.\n",
    "    y=nltk.word_tokenize(x)\n",
    "    temp=[]\n",
    "    for i in y:\n",
    "      if i in stopwords.words(\"english\"): \n",
    "         pass\n",
    "      else:\n",
    "         temp.append(i)\n",
    "    my_new_string=' '.join(temp)\n",
    "#---------------------------------------------------------------------------------------------------------------------------        \n",
    "    #Lemmatizing the data.\n",
    "    c=nltk.word_tokenize(my_new_string)\n",
    "    my_list=[]\n",
    "    for i in c:\n",
    "        my_list.append(lemmatizer.lemmatize(i))\n",
    "        my_new_string_second=' '.join(my_list)\n",
    "#---------------------------------------------------------------------------------------------------------------------------            \n",
    "    #Stemming the data.\n",
    "        d=nltk.word_tokenize(my_new_string_second)\n",
    "    my_list_stemmer=[]\n",
    "    for i in d:\n",
    "        my_list_stemmer.append(sn_stemmer.stem(i))\n",
    "        final=' '.join(my_list_stemmer)\n",
    "#---------------------------------------------------------------------------------------------------------------------------  \n",
    "    #Removing the numbers from the data.\n",
    "        temp=[]\n",
    "    for i in final:\n",
    "      if i.isdigit(): \n",
    "         pass\n",
    "      else:\n",
    "        temp.append(i)\n",
    "    hi=''.join(temp)\n",
    "    return (hi.lower()) #Converting the data to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "112dd652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i go nd rasesh choudhari'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing the function.\n",
    "\n",
    "datacleaning('I goes to 2nd and i am Rasesh Choudhary')\n",
    "\n",
    "# Converted to lower case.\n",
    "#stop words removes\n",
    "# lemmatized and stemmed\n",
    "# Number removed\n",
    "# The function works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "25559887",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned_review']=df['Review'].apply(datacleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "18393b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "      <th>Cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>wow ... love place .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>crust good .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>not tasti textur nasti .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>stop late may bank holiday rick steve recommen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>the select menu great price .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "      <td>i think food flavor textur lack .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "      <td>appetit instant gone .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "      <td>overal i impress would go back .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>the whole experi underwhelm , i think ll go ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>0</td>\n",
       "      <td>then , i n't wast enough life , pour salt woun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Review  Liked  \\\n",
       "0                             Wow... Loved this place.      1   \n",
       "1                                   Crust is not good.      0   \n",
       "2            Not tasty and the texture was just nasty.      0   \n",
       "3    Stopped by during the late May bank holiday of...      1   \n",
       "4    The selection on the menu was great and so wer...      1   \n",
       "..                                                 ...    ...   \n",
       "995  I think food should have flavor and texture an...      0   \n",
       "996                           Appetite instantly gone.      0   \n",
       "997  Overall I was not impressed and would not go b...      0   \n",
       "998  The whole experience was underwhelming, and I ...      0   \n",
       "999  Then, as if I hadn't wasted enough of my life ...      0   \n",
       "\n",
       "                                        Cleaned_review  \n",
       "0                                 wow ... love place .  \n",
       "1                                         crust good .  \n",
       "2                             not tasti textur nasti .  \n",
       "3    stop late may bank holiday rick steve recommen...  \n",
       "4                        the select menu great price .  \n",
       "..                                                 ...  \n",
       "995                  i think food flavor textur lack .  \n",
       "996                             appetit instant gone .  \n",
       "997                   overal i impress would go back .  \n",
       "998  the whole experi underwhelm , i think ll go ni...  \n",
       "999  then , i n't wast enough life , pour salt woun...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c9c6d",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1864c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "82034e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y=df['Liked']\n",
    "df_x=df['Cleaned_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "30354232",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(df_x,df_y,test_size=0.33,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9c1b27f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[590                            great place fo take eat .\n",
       " 395    to summar ... food incred , nay , transcend .....\n",
       " 414                  i take littl bad servic food suck .\n",
       " 666                                   the staff attent .\n",
       " 987              it lack flavor , seem undercook , dri .\n",
       "                              ...                        \n",
       " 934         the place fair clean food simpli n't worth .\n",
       " 910    i serious believ owner mani unexperienc employ...\n",
       " 386                   i come back everi time i 'm vega .\n",
       " 754    main thing i n't enjoy crowd older crowd , aro...\n",
       " 429    these nicest restaur owner i ve ever come acro...\n",
       " Name: Cleaned_review, Length: 800, dtype: object,\n",
       " 407             food realli good i got full petti fast .\n",
       " 112    this realli fantast thai restaur definit worth...\n",
       " 183                                    it probabl dirt .\n",
       " 804                          they golden-crispi delici .\n",
       " 698                          needless say , never back .\n",
       "                              ...                        \n",
       " 965                 it 's not hard make decent hamburg .\n",
       " 999    then , i n't wast enough life , pour salt woun...\n",
       " 860    this place pretti good , nice littl vibe resta...\n",
       " 219                           - food rich order accord .\n",
       " 798                                       mediocr food .\n",
       " Name: Cleaned_review, Length: 200, dtype: object,\n",
       " 590    1\n",
       " 395    1\n",
       " 414    0\n",
       " 666    1\n",
       " 987    0\n",
       "       ..\n",
       " 934    0\n",
       " 910    0\n",
       " 386    1\n",
       " 754    0\n",
       " 429    1\n",
       " Name: Liked, Length: 800, dtype: int64,\n",
       " 407    1\n",
       " 112    1\n",
       " 183    0\n",
       " 804    1\n",
       " 698    0\n",
       "       ..\n",
       " 965    0\n",
       " 999    0\n",
       " 860    1\n",
       " 219    1\n",
       " 798    0\n",
       " Name: Liked, Length: 200, dtype: int64]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_split(df_x,df_y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "54a6f660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#initializing the vector.\n",
    "hi_vectorize = CountVectorizer()\n",
    "hi_vectorize.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "62b49ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['absolut', 'absolutley', 'accid', 'accomod', 'accord', 'account',\n",
       "       'acknowledg', 'actual', 'ad', 'afford', 'after', 'afternoon',\n",
       "       'again', 'ago', 'al', 'ala', 'all', 'allergi', 'almost', 'alon',\n",
       "       'also', 'although', 'alway', 'amaz', 'ambianc', 'ambienc',\n",
       "       'amount', 'ampl', 'an', 'and', 'andddd', 'ani', 'anoth', 'anymor',\n",
       "       'anyon', 'anyth', 'anytim', 'anyway', 'apart', 'apolog', 'app',\n",
       "       'appal', 'appar', 'appeal', 'appet', 'appetit', 'appl', 'are',\n",
       "       'area', 'arepa', 'aria', 'around', 'array', 'arriv', 'as', 'ask',\n",
       "       'assur', 'at', 'ate', 'atmospher', 'atmosphere', 'attach',\n",
       "       'attack', 'attent', 'attitud', 'auju', 'authent', 'averag',\n",
       "       'avocado', 'avoid', 'aw', 'away', 'awesom', 'awkward', 'babi',\n",
       "       'back', 'bacon', 'bad', 'bagel', 'bakeri', 'bamboo', 'bar', 'bare',\n",
       "       'bargain', 'bartend', 'basebal', 'basic', 'bathroom', 'batter',\n",
       "       'bay', 'bbq', 'bean', 'beat', 'beauti', 'becom', 'beef', 'been',\n",
       "       'beer', 'begin', 'behind'], dtype=object)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi_vectorize.get_feature_names_out()[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "93409cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_hi = hi_vectorize.transform(x_train)\n",
    "x_train_vector=x_train_hi.toarray()\n",
    "x_train_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e4641917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi_vectorize.fit(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5bd71efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['accommod', 'ach', 'acknowledg', 'across', 'actual', 'after',\n",
       "       'ahead', 'airlin', 'airport', 'albondiga', 'all', 'almond', 'also',\n",
       "       'although', 'alway', 'amaz', 'ambienc', 'amount', 'an', 'and',\n",
       "       'angri', 'annoy', 'anoth', 'anticip', 'anytim', 'approv', 'area',\n",
       "       'around', 'arriv', 'articl', 'as', 'ask', 'at', 'atmospher',\n",
       "       'atroci', 'attent', 'authent', 'averag', 'avoid', 'aw', 'awesom',\n",
       "       'ayc', 'az', 'baba', 'bachi', 'back', 'bacon', 'bad', 'baklava',\n",
       "       'ball', 'banana', 'bank', 'bar', 'bare', 'bartend', 'base',\n",
       "       'batch', 'bathroom', 'batter', 'bay', 'be', 'beateous', 'beauti',\n",
       "       'beef', 'beer', 'befor', 'behind', 'bellagio', 'best', 'better',\n",
       "       'beyond', 'big', 'bigger', 'bing', 'bit', 'bite', 'black', 'bland',\n",
       "       'blandest', 'block', 'bloddi', 'bloodiest', 'blow', 'boba', 'boil',\n",
       "       'both', 'box', 'boyfriend', 'bread', 'breakfast', 'brisket',\n",
       "       'brought', 'brushfir', 'buck', 'buffet', 'bug', 'burger', 'burn',\n",
       "       'busi', 'but'], dtype=object)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi_vectorize.get_feature_names_out()[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ae9a29f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_hi = hi_vectorize.transform(x_test)\n",
    "x_test_vector=x_test_hi.toarray()\n",
    "x_test_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c7665cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "28f5188a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "109c7c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "02c954cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=le.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8e79bf4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7f9b371d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a6ba5f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6b70b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "795f3221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
